{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 style=\"text-align: center;\">Lista 6 - Exercícios Redes Neurais:</h1>\n",
    "<h2 style=\"text-align: center;\">\n",
    "    <a href=\"https://github.com/Jodavid\" target=\"_blank\" style=\"color: blue; text-decoration: underline;\">\n",
    "        Prof. Dr. Jodavid Ferreira\n",
    "    </a>\n",
    "</h2>\n",
    "\n",
    "*Discente:*\n",
    "\n",
    "    *   Gabriel D'assumpção de Carvalho\n",
    "\n",
    "*Data:* 09/09/2024\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **O que é uma rede neural totalmente conectada (fully connected neural network)?**\n",
    "\n",
    "Uma rede neural (nn) totalmente conectada é um tipo de arquitetura de rede neural em que cada neurônio de uma camada está conectado a todos neurônios da camada seguinte. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Como o Dropout funciona em uma rede neural e por que ele é importante?**\n",
    "\n",
    "O dropout é um parâmetro proposto na arquitetura da rede que tem como proposito desativar uma porcentagem (X%) dos neurônios de uma camada durante o treinamento. Isso significa que a cada cada época, diferentes subconjuntos de neurônios vão ser ignorados, o que força a rede a não depender de combinações específicas de neurônios. O objetivo é reduzir o overfiting, melhorando a capacidade da rede de generalizar para novas observações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **O que significa overfitting em redes neurais e como o Dropout ajuda a preveni-lo?**\n",
    "\n",
    "Overfitting é um problema que acontece quando um modelo se ajusta muito bem para os dados de treinamento, aprendendo até mesmo o ruído e variação desse conjunto de dados. Como resultado o modelo perde a capacidade de generalizar para novos dados, apresentando um desempenho muito inferior nos dados de validação e/ou de teste. Isso é frequentemente identificado quando o modelo possui uma acurácia alta nos dados de treinamento mas baixa nos dados de validação e/ou teste. \n",
    "\n",
    "O dropout ajuda a previnir o overfitting ao desativar aleatoriamente uma fração dos neurônios em cada época de treinamento, forçando a rede a não depender excessivamente de uma combinação específica de neurônios. Dessa maneira, a rede é forçada a aprender representações mais generalizadas e robustas, distribuindo o erro np backpropagation de uma melhor forma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Qual é o impacto do uso de uma taxa de Dropout alta versus uma taxa de Dropout baixa em uma rede neural?**\n",
    "\n",
    "Uma alta taxa de dropout (por exemplo 50% ou mais) pode levar ao underfitting, que ocorre quando o modelo não consegue capturar o padrão dos dados. isso acontece porque, com muitos neurônios desativados o modelo perde a capacidade de aprender as relações entre as variáveis na fase de treinamento. Como resultado, o desempenho do modelo fica ruim tanto nos dados de treinamento como no teste.\n",
    "\n",
    "Por outro lado, um dropout baixo a moderado (entre 20% a 30%) são geralmente eficazes para evitar o overfitting. Essas taxas ajudam ao modelo a não se ajustar demais ao conjunto de dados de treinamento, sem comprometer a sua capacidade de aprendizado. A taxa de dropout ideal vai depender da necessidade de reduzir o overfitting e o risco em cair em um underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Como o Dropout é implementado durante a fase de inferência (teste/predição) de uma rede neural?**\n",
    "\n",
    "Quando o dropout é utilizado duranto o treinamento, uma porcentagem (por exemplo 50%) dos neurônios são desativados, para evitar overfitting. No entando, durante a fase de teste/predição, o dropout é desligado, ou seja, todos os neurônios estão ativos.\n",
    "\n",
    "Para compensar o fato de que, no treinamento nem todos os neurônios estavam ativados, as ativações dos neurônios vão ser ajustados na fase de inferência. Isso é feito por meio da técnica chamada de **inverted dropout**, onde a saída de cada neurônio é multiplicada por uma fração correspondende a taxa de dropout usada durante o treinamento. Por exemplo, se a taxa de dropout utilizada foi de 50%, as ativações são multiplicadas por 0.5 para manter a corência com a fase de treinamento onde o modelo desativava 50% dos neurônios.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
